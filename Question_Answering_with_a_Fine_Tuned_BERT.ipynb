{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d59f6a3560440ba8d9ab79070d1b303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3121e4e9928941b49e2cf01c498e9c1a",
              "IPY_MODEL_175c3cc44b6d4c9cb864115e4d4e8432",
              "IPY_MODEL_17fe74fc2e7e4e68916328328b9d5a00"
            ],
            "layout": "IPY_MODEL_f1541be46351431ba7e7bc329489ea29"
          }
        },
        "5fee3760a7c84229830724fe11268682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80507d7de9f44e8396ab5d0fcf6aa7ff",
              "IPY_MODEL_bc01e44875d245d19f0cfccd4179a9dc",
              "IPY_MODEL_cad8c41dc9fe495c9928d2c20e138f7e"
            ],
            "layout": "IPY_MODEL_42899be1565246ef975b6f0e129f495e"
          }
        },
        "6a57049609434605bd398a1a96d230d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59849d8baf834169884cb4c4c482fd2b",
              "IPY_MODEL_5a2de68e8e29412b926e19caecc29ba7",
              "IPY_MODEL_00a94bc8898f46e2b0bc6637599eac8d"
            ],
            "layout": "IPY_MODEL_a05bc66af4ff4d6c945c5e272dd9457c"
          }
        },
        "59849d8baf834169884cb4c4c482fd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d2e25a0a7e84d98849500de488ffd52",
            "placeholder": "​",
            "style": "IPY_MODEL_a4dc1bcf59ba4ac6b146d51de8744dd0",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "5a2de68e8e29412b926e19caecc29ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8f9b3de01e14d6ea301f14c0b699b66",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_797562dd03684f868c9b2f57168570fd",
            "value": 231508
          }
        },
        "00a94bc8898f46e2b0bc6637599eac8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f513e2adf5d94f558909befbf8735a96",
            "placeholder": "​",
            "style": "IPY_MODEL_3a02d094b84145e5b2b36d28fd66d275",
            "value": " 226k/226k [00:00&lt;00:00, 826kB/s]"
          }
        },
        "a05bc66af4ff4d6c945c5e272dd9457c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d2e25a0a7e84d98849500de488ffd52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4dc1bcf59ba4ac6b146d51de8744dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f9b3de01e14d6ea301f14c0b699b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "797562dd03684f868c9b2f57168570fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f513e2adf5d94f558909befbf8735a96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a02d094b84145e5b2b36d28fd66d275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "784183c7ced9401c990544d6e793d0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f92bb256ee194002afd737df1ad0a48c",
              "IPY_MODEL_e23f41c76e754662ae7fe837fbc5d40e",
              "IPY_MODEL_514476f1c8d04457b7efb1c065396ac6"
            ],
            "layout": "IPY_MODEL_cb3acd274ac744d8bb644e131da3740d"
          }
        },
        "f92bb256ee194002afd737df1ad0a48c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e449cf121d74bc7b75adf271c1ec483",
            "placeholder": "​",
            "style": "IPY_MODEL_76564302b82b401c982b7b8503edb1ba",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "e23f41c76e754662ae7fe837fbc5d40e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_303559125afc4a578a2cac4f7226ffe1",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c35226917fb4ce7b53540aa1f57643a",
            "value": 28
          }
        },
        "514476f1c8d04457b7efb1c065396ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b09051a76dbc49debaa16a7b88d22aeb",
            "placeholder": "​",
            "style": "IPY_MODEL_5b8a41cada2a4fb1bd7ebbbbd799c502",
            "value": " 28.0/28.0 [00:00&lt;00:00, 596B/s]"
          }
        },
        "cb3acd274ac744d8bb644e131da3740d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e449cf121d74bc7b75adf271c1ec483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76564302b82b401c982b7b8503edb1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "303559125afc4a578a2cac4f7226ffe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c35226917fb4ce7b53540aa1f57643a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b09051a76dbc49debaa16a7b88d22aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b8a41cada2a4fb1bd7ebbbbd799c502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-1zl5XdYInf"
      },
      "source": [
        "# Question Answering with a Fine-Tuned BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_mAnIPKaXyw"
      },
      "source": [
        "This notebook has 2 parts:\n",
        "\n",
        "**Part 1** Apply BERT to QA, and illustrate the details.\n",
        "\n",
        "**Part 2** Download a model that's *already been fine-tuned* for question answering, and try it out on our own text! \n",
        "\n",
        "For something like text classification, we definitely want to fine-tune BERT on your own dataset. For question answering, however, it seems like you may be able to get decent results using a model that's already been fine-tuned on the SQuAD benchmark. In this Notebook, we'll do exactly that, and see that it performs well on text that wasn't in the SQuAD dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2bUvKUffHNY"
      },
      "source": [
        "# Part 1: How BERT is applied to Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su7fixBdiUex"
      },
      "source": [
        "## The SQuAD v1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT5ESKDxfnLf"
      },
      "source": [
        "\"Question Answering\" System is an application of BERT,where BERT is applied to the Stanford Question Answering Dataset (SQuAD).\n",
        "\n",
        "Given a question, and *a passage of text containing the answer*, BERT needs to highlight the \"span\" of text corresponding to the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xN5f1bxf6K_"
      },
      "source": [
        "## BERT Input Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctum5SK6f9uP"
      },
      "source": [
        "To feed a QA task into BERT, we pack both the question and the reference text into the input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaPCFHgSOO37"
      },
      "source": [
        "\n",
        "[![Input format for QA](https://drive.google.com/uc?export=view&id=1dfgTaE_SABpr2blqwTjq9PTyhYabO8_m)](https://drive.google.com/uc?export=view&id=1dfgTaE_SABpr2blqwTjq9PTyhYabO8_m)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-SS0PucOQdt"
      },
      "source": [
        "\n",
        "The two pieces of text are separated by the special `[SEP]` token. \n",
        "\n",
        "BERT also uses \"Segment Embeddings\" to differentiate the question from the reference text. These are simply two embeddings (for segments \"A\" and \"B\") that BERT learned, and which it adds to the token embeddings before feeding them into the input layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "457VPa20fZzY"
      },
      "source": [
        "# Part 2: Example Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wpCLgVCkki5"
      },
      "source": [
        "In the example code below, we'll be downloading a model that's *already been fine-tuned* for question answering, and try it out on our own text.\n",
        "\n",
        "If you do want to fine-tune on your own dataset, it is possible to fine-tune BERT for question answering yourself.\n",
        "\n",
        "> Note: The example code in this Notebook is a commented and expanded version of the short example provided in the `transformers` documentation [here](https://huggingface.co/transformers/model_doc/bert.html?highlight=bertforquestionanswering#transformers.BertForQuestionAnswering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVq-TuylYRDW"
      },
      "source": [
        "## 1. Install huggingface transformers library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9nhy3PzGQ44"
      },
      "source": [
        "This example uses the `transformers` [library](https://github.com/huggingface/transformers/) by huggingface. We'll start by installing the package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQl0MMrOGIup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43954d58-7e5a-474d-b480-f6768ba70302"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ONLrgJK99TQ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WThOUtpYvG-"
      },
      "source": [
        "## 2. Load Fine-Tuned BERT-large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaweLnNXGhTY"
      },
      "source": [
        "For Question Answering we use the `BertForQuestionAnswering` class from the `transformers` library.\n",
        "\n",
        "This class supports fine-tuning, but for this example we will keep things simpler and load a BERT model that has already been fine-tuned for the SQuAD benchmark.\n",
        "\n",
        "The `transformers` library has a large collection of pre-trained models which you can reference by name and load easily. The full list is in their documentation [here](https://huggingface.co/transformers/pretrained_models.html).\n",
        "\n",
        "For Question Answering, they have a version of BERT-large that has already been fine-tuned for the SQuAD benchmark. \n",
        "\n",
        "BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mnv95sX-U9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "3d59f6a3560440ba8d9ab79070d1b303",
            "5fee3760a7c84229830724fe11268682"
          ]
        },
        "outputId": "4c3129fa-a7ad-40ec-d4ec-ba845165cf8b"
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d59f6a3560440ba8d9ab79070d1b303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fee3760a7c84229830724fe11268682"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8imoOxoqGZ0h"
      },
      "source": [
        "Load the tokenizer as well. \n",
        "\n",
        "Side note: Apparently the vocabulary of this model is identicaly to the one in bert-base-uncased. You can load the tokenizer from `bert-base-uncased` and that works just as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFQ5f7gv-RBH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "6a57049609434605bd398a1a96d230d0",
            "59849d8baf834169884cb4c4c482fd2b",
            "5a2de68e8e29412b926e19caecc29ba7",
            "00a94bc8898f46e2b0bc6637599eac8d",
            "a05bc66af4ff4d6c945c5e272dd9457c",
            "5d2e25a0a7e84d98849500de488ffd52",
            "a4dc1bcf59ba4ac6b146d51de8744dd0",
            "c8f9b3de01e14d6ea301f14c0b699b66",
            "797562dd03684f868c9b2f57168570fd",
            "f513e2adf5d94f558909befbf8735a96",
            "3a02d094b84145e5b2b36d28fd66d275",
            "784183c7ced9401c990544d6e793d0c6",
            "f92bb256ee194002afd737df1ad0a48c",
            "e23f41c76e754662ae7fe837fbc5d40e",
            "514476f1c8d04457b7efb1c065396ac6",
            "cb3acd274ac744d8bb644e131da3740d",
            "8e449cf121d74bc7b75adf271c1ec483",
            "76564302b82b401c982b7b8503edb1ba",
            "303559125afc4a578a2cac4f7226ffe1",
            "3c35226917fb4ce7b53540aa1f57643a",
            "b09051a76dbc49debaa16a7b88d22aeb",
            "5b8a41cada2a4fb1bd7ebbbbd799c502"
          ]
        },
        "outputId": "44c63573-9de4-44ae-ed7c-afa0b1eed4c7"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a57049609434605bd398a1a96d230d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "784183c7ced9401c990544d6e793d0c6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I__1ubvcZYow"
      },
      "source": [
        "## 3. Ask a Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8MQ7b-GJIcM"
      },
      "source": [
        "Now we're ready to feed in an example!\n",
        "\n",
        "A QA example consists of a question and a passage of text containing the answer to that question.\n",
        "\n",
        "Let's try an example using the text in this tutorial!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWzZP4EN-Zxg"
      },
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llLvxhScKLZn"
      },
      "source": [
        "We'll need to run the BERT tokenizer against both the `question` and the `answer_text`. To feed these into BERT, we actually concatenate them together and place the special [SEP] token in between.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYoX33CfKGsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5060160-16c6-4ac1-e991-419b2fcd5ff4"
      },
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input has a total of 70 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNRVuaKSNFG8"
      },
      "source": [
        "Just to see exactly what the tokenizer is doing, let's print out the tokens with their IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iow838yPNDTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb35579-9f3f-4248-8c48-bf4d9bdaeb4b"
      },
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]           101\n",
            "how           2,129\n",
            "many          2,116\n",
            "parameters   11,709\n",
            "does          2,515\n",
            "bert         14,324\n",
            "-             1,011\n",
            "large         2,312\n",
            "have          2,031\n",
            "?             1,029\n",
            "\n",
            "[SEP]           102\n",
            "\n",
            "bert         14,324\n",
            "-             1,011\n",
            "large         2,312\n",
            "is            2,003\n",
            "really        2,428\n",
            "big           2,502\n",
            ".             1,012\n",
            ".             1,012\n",
            ".             1,012\n",
            "it            2,009\n",
            "has           2,038\n",
            "24            2,484\n",
            "-             1,011\n",
            "layers        9,014\n",
            "and           1,998\n",
            "an            2,019\n",
            "em            7,861\n",
            "##bed         8,270\n",
            "##ding        4,667\n",
            "size          2,946\n",
            "of            1,997\n",
            "1             1,015\n",
            ",             1,010\n",
            "02            6,185\n",
            "##4           2,549\n",
            ",             1,010\n",
            "for           2,005\n",
            "a             1,037\n",
            "total         2,561\n",
            "of            1,997\n",
            "340          16,029\n",
            "##m           2,213\n",
            "parameters   11,709\n",
            "!               999\n",
            "altogether   10,462\n",
            "it            2,009\n",
            "is            2,003\n",
            "1             1,015\n",
            ".             1,012\n",
            "34            4,090\n",
            "##gb         18,259\n",
            ",             1,010\n",
            "so            2,061\n",
            "expect        5,987\n",
            "it            2,009\n",
            "to            2,000\n",
            "take          2,202\n",
            "a             1,037\n",
            "couple        3,232\n",
            "minutes       2,781\n",
            "to            2,000\n",
            "download      8,816\n",
            "to            2,000\n",
            "your          2,115\n",
            "cola         15,270\n",
            "##b           2,497\n",
            "instance      6,013\n",
            ".             1,012\n",
            "\n",
            "[SEP]           102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm208EApN16k"
      },
      "source": [
        "We've concatenated the `question` and `answer_text` together, but BERT still needs a way to distinguish them. BERT has two special \"Segment\" embeddings, one for segment \"A\" and one for segment \"B\". Before the word embeddings go into the BERT layers, the segment A embedding needs to be added to the `question` tokens, and the segment B embedding needs to be added to each of the `answer_text` tokens. \n",
        "\n",
        "These additions are handled for us by the `transformer` library, and all we need to do is specify a '0' or '1' for each token. \n",
        "\n",
        "Note: In the `transformers` library, huggingface likes to call these `token_type_ids`, but I'm going with `segment_ids` since this seems clearer, and is consistent with the BERT paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ7PRx2dKqFN"
      },
      "source": [
        "# Search the input_ids for the first instance of the `[SEP]` token.\n",
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of segment A tokens includes the [SEP] token istelf.\n",
        "num_seg_a = sep_index + 1\n",
        "\n",
        "# The remainder are segment B.\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "# Construct the list of 0s and 1s.\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNwhEw0kQPBN"
      },
      "source": [
        "We're ready to feed our example into the model!\n",
        "\n",
        "Note: The result object is of type [QuestionAnsweringModelOutput](https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.QuestionAnsweringModelOutput)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQiKr6Aw-YTg"
      },
      "source": [
        "# Run our example through the model.\n",
        "outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                             token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                             return_dict=True) \n",
        "\n",
        "start_scores = outputs.start_logits\n",
        "end_scores = outputs.end_logits\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBdS_QkIbDzh"
      },
      "source": [
        "Now we can highlight the answer just by looking at the most probable start and end words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeUQ44hAJmn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb062c7f-e8a3-4568-d034-c7f7fade849c"
      },
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_scores)\n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \"340 ##m\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twMUWmr2brRw"
      },
      "source": [
        "It got it right! Awesome :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cERYCGKMbOXX"
      },
      "source": [
        "> *Side Note: It's a little naive to pick the highest scores for start and end--what if it predicts an end word that's before the start word?! The correct implementation is to pick the highest total score for which end >= start.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6j2znkwXYsn"
      },
      "source": [
        "With a little more effort, we can reconstruct any words that got broken down into subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khral6HZXCuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95aad24-4292-4271-b30f-b2bcbc57797f"
      },
      "source": [
        "# Start with the first token.\n",
        "answer = tokens[answer_start]\n",
        "\n",
        "# Select the remaining answer tokens and join them with whitespace.\n",
        "for i in range(answer_start + 1, answer_end + 1):\n",
        "    \n",
        "    # If it's a subword token, then recombine it with the previous token.\n",
        "    if tokens[i][0:2] == '##':\n",
        "        answer += tokens[i][2:]\n",
        "    \n",
        "    # Otherwise, add a space then the token.\n",
        "    else:\n",
        "        answer += ' ' + tokens[i]\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \"340m\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWtcRpPef-Ce"
      },
      "source": [
        "Turn the QA process into a function so we can easily try out other examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH8NbBlsfxZ_"
      },
      "source": [
        "def answer_question(question, answer_text):\n",
        "    '''\n",
        "    Takes a `question` string and an `answer_text` string (which contains the\n",
        "    answer), and identifies the words within the `answer_text` that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    # Report how long the input sequence is.\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example through the model.\n",
        "    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                    return_dict=True) \n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVlKTK-njWrX"
      },
      "source": [
        "As our reference text, I've taken the Abstract of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4VPq6FdjxyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394eec47-1c2a-4c4e-e987-c1e6a0b06fa7"
      },
      "source": [
        "import textwrap\n",
        "\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
        "\n",
        "print(wrapper.fill(bert_abstract))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We introduce a new language representation model called BERT, which stands for\n",
            "Bidirectional Encoder Representations from Transformers. Unlike recent language\n",
            "representation models (Peters et al., 2018a; Radford et al., 2018), BERT is\n",
            "designed to pretrain deep bidirectional representations from unlabeled text by\n",
            "jointly conditioning on both left and right context in all layers. As a result,\n",
            "the pre-trained BERT model can be finetuned with just one additional output\n",
            "layer to create state-of-the-art models for a wide range of tasks, such as\n",
            "question answering and language inference, without substantial taskspecific\n",
            "architecture modifications. BERT is conceptually simple and empirically\n",
            "powerful. It obtains new state-of-the-art results on eleven natural language\n",
            "processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute\n",
            "improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1\n",
            "question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD\n",
            "v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEB654YCknYv"
      },
      "source": [
        "-----------------------------\n",
        "Ask BERT what its name stands for (the answer is in the first sentence of the abstract)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfntqRCBegGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c9f63d-2568-45a5-8103-b79d25802ad7"
      },
      "source": [
        "question = \"What is the full form of BERT?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query has 255 tokens.\n",
            "\n",
            "Answer: \"bidirectional encoder representations from transformers\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6HcijzxkTO9"
      },
      "source": [
        "---------------------\n",
        "Ask BERT about example applications of itself :)\n",
        "\n",
        "The answer to the question comes from this passage from the abstract: \n",
        "\n",
        "> \"...BERT model can be finetuned with just one additional output\n",
        "layer to create state-of-the-art models for **a wide range of tasks, such as\n",
        "question answering and language inference,** without substantial taskspecific\n",
        "architecture modifications.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVNVGN5-gI06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3983437c-58d9-49c2-e589-bcca63298b78"
      },
      "source": [
        "question = \"Give me some example applications of BERT?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query has 255 tokens.\n",
            "\n",
            "Answer: \"question answering and language inference\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-2YC36L3IeB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}